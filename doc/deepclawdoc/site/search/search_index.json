{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The DeepClaw Benchmark The DeepClaw is a benchmarking model zoo that functions as a Reconfigurable Robotic Manipulation System for Robot Learning. The main homepage for Julia can be found at deepclaw.ancorasir.com . This is the GitHub repository of DeepClaw source code, including instructions for installing and using DeepClaw, below. Resources Homepage: https://deepclaw.ancorasir.com/ Documentation: https://bionicdl-sustech.github.io/DeepClawBenchmark/_build/html/index.html Paper explaining DeepClaw: arXiv ( BibTex ) Papers using DeepClaw: arXiv:2003.01584 [cs.RO] arXiv:2003.01583 [cs.RO] arXiv:2003.01582 [cs.RO] Installation from Source As of now, DeepClaw framework has been tested with Python 2.7 and Ubuntu 16.04 LTS, with a near-future plan to update to Python 3.x with Ubunti 18.04 LTS. Virtual Environment We recommend using a virtual environment (such as virtualenv) to manage DeepClaw. Install virtualenv. $ pip install -U virtualenv Create a new virtual environment. $ virtualenv -p /usr/bin/python2.7 ~/DCvenv Activate or retreat from virtual environment. $ source ~/DCvenv/bin/activate # activate virtual environment $ deactivate # retreat from virtual environment Requirements In the current realse, support is provided for a baselone setup with UR10e, HandE, and RealSense D435. The depenences of DeepClaw are showed below: python-pip install numpy==1.16.2 opencv-python==3.3.1.11 scipy==1.2.2 tensorflow==1.12.0 open3d RealSense SDK (https://www.intelrealsense.com/developers/), pyrealsense2 Code Organization The DeepClaw code is organized as follows: configs/ configuration for robotic station for manipulation tasks. deepclaw/drivers/ drivers for various robotic hardware, i.e. ur, franka, aubo. deepclaw/models/ model zoo for segmentation, classification, pick planning, and motion planning. deepclaw/utils/ server setup with dockers and client setup for laptops (x86) and jetson (arm). projects/proj_trashSort a sample project to run deepclaw for sorting trash. datasets/trash description of trash sorting dataset docs/ description of this document as a manual. TODO list in the next update: projects/proj_claw a sample project to run deepclaw in arcade claw game. projects/proj_jigsaw a sample project to run deepclaw in jigsaw game. projects/proj_oxTTT a sample project to run deepclaw in tic-tac-toe game. datasets/toys description of the toy dataset datasets/jigsaw description of jigsaw game pieces dataset datasets/mnist description of mnist dataset Bibliography arXiv","title":"Home"},{"location":"#the-deepclaw-benchmark","text":"The DeepClaw is a benchmarking model zoo that functions as a Reconfigurable Robotic Manipulation System for Robot Learning. The main homepage for Julia can be found at deepclaw.ancorasir.com . This is the GitHub repository of DeepClaw source code, including instructions for installing and using DeepClaw, below.","title":"The DeepClaw Benchmark"},{"location":"#resources","text":"Homepage: https://deepclaw.ancorasir.com/ Documentation: https://bionicdl-sustech.github.io/DeepClawBenchmark/_build/html/index.html Paper explaining DeepClaw: arXiv ( BibTex ) Papers using DeepClaw: arXiv:2003.01584 [cs.RO] arXiv:2003.01583 [cs.RO] arXiv:2003.01582 [cs.RO]","title":"Resources"},{"location":"#installation-from-source","text":"As of now, DeepClaw framework has been tested with Python 2.7 and Ubuntu 16.04 LTS, with a near-future plan to update to Python 3.x with Ubunti 18.04 LTS.","title":"Installation from Source"},{"location":"#virtual-environment","text":"We recommend using a virtual environment (such as virtualenv) to manage DeepClaw. Install virtualenv. $ pip install -U virtualenv Create a new virtual environment. $ virtualenv -p /usr/bin/python2.7 ~/DCvenv Activate or retreat from virtual environment. $ source ~/DCvenv/bin/activate # activate virtual environment $ deactivate # retreat from virtual environment","title":"Virtual Environment"},{"location":"#requirements","text":"In the current realse, support is provided for a baselone setup with UR10e, HandE, and RealSense D435. The depenences of DeepClaw are showed below: python-pip install numpy==1.16.2 opencv-python==3.3.1.11 scipy==1.2.2 tensorflow==1.12.0 open3d RealSense SDK (https://www.intelrealsense.com/developers/), pyrealsense2","title":"Requirements"},{"location":"#code-organization","text":"The DeepClaw code is organized as follows: configs/ configuration for robotic station for manipulation tasks. deepclaw/drivers/ drivers for various robotic hardware, i.e. ur, franka, aubo. deepclaw/models/ model zoo for segmentation, classification, pick planning, and motion planning. deepclaw/utils/ server setup with dockers and client setup for laptops (x86) and jetson (arm). projects/proj_trashSort a sample project to run deepclaw for sorting trash. datasets/trash description of trash sorting dataset docs/ description of this document as a manual. TODO list in the next update: projects/proj_claw a sample project to run deepclaw in arcade claw game. projects/proj_jigsaw a sample project to run deepclaw in jigsaw game. projects/proj_oxTTT a sample project to run deepclaw in tic-tac-toe game. datasets/toys description of the toy dataset datasets/jigsaw description of jigsaw game pieces dataset datasets/mnist description of mnist dataset","title":"Code Organization"},{"location":"#bibliography","text":"arXiv","title":"Bibliography"},{"location":"about/","text":"Scyrum an prima rubent poteratque ulla celebrare Orba formosa Lorem markdownum inventa vos quis sinistra moventur harundine nubibus, in movere dedit sacer magnas opis aetatis! Patitur has freta pulchros centumque, ubi altera dearum. Illae venit gerit ortum inpia ingens, femina forent tremulis; victor uror putares prolem . Fragmina si ubi dea genuit es viso quem, vetustas et quid, rex contulerat lentisciferumque insilit. Trepidus locutus corpora ignes! Nec enim fecere Nam tarda et alter non Ubi regni solebat Gentes viscera precor rogat In clausit patrem quamvis verbere Nunc ore mater armenta est lumine, quam ter Coniugis a cumque, amictu, atra nudos fugiunt Achivi geminos. Nec est Curibusque sperni saxoque et raptae, nulli! Tibi sub adest magnis agnovit quaecumque somnos flammas inpia concutiens, conscendit At cum prohibete. Tibi icta saevior simillimus loqui; sed herba ira erat viribus lumen pendeat modo senex porrigeret quaesita mea; arboreo. Ululatibus primo renasci Pars edentem sulphura nati prodest, ego quo, atrae inpius saxum. Et e ramos volucrem volenti ritu, inserere domos fronti famam. Sidere esse lumina Orphne Somnia donisque liquerat dixit et agrestis ergo, mente tamen. Est est Domoque vocat Tibi tecum sistetur Cecrope natos Phegeius Dolos fulmina Vita sub violave quae Pavidus o in alis Simul si nec fallebat fraterno parenti in culpa matertera inde planxere enim Ciconum ne tempora meorum praecipitem mersit. Obscuraque daedalus Iunone; dulce in quantas erat sumit animae cui undis ad latronis, inpune et. Sagitta virgo turpius: ursos quis spiritus pisa vatis fatale, mortis vidisti, in, sed Hactenus pendet. In nubimus qui, Somnia cecidisse alantqueimperat quaeritur contende alimentaque et in resolvit habentem tantos putes: femina levis suus? Labori et meorum fuissem membris. Omnes Proserpina vocassent apro excubias ulvaeque Pleuronius et mugiat modo placat quod. Ter thalamos relinque, putes tegis partem ferat mihi , nullamque saepe, virosque caelesti sub et. Latratibus notissima violenta ad veri , sub de est, de formosi adspicere plus orbem conviciaque videtur. Frater dederat nunc, meritoque urebat et sed; genialis draconi certamine flagrantem tabo; Mermeros. Lumen tu sed rapinae lotos hominis ter, erat dempto in aevum nimiumque ex neci simul manusque Tarpeias callem insistere. Virgo non quibus tamen rursus, Patareaque, nec reppulit putes et septem.","title":"Scyrum an prima rubent poteratque ulla celebrare"},{"location":"about/#scyrum-an-prima-rubent-poteratque-ulla-celebrare","text":"","title":"Scyrum an prima rubent poteratque ulla celebrare"},{"location":"about/#orba-formosa","text":"Lorem markdownum inventa vos quis sinistra moventur harundine nubibus, in movere dedit sacer magnas opis aetatis! Patitur has freta pulchros centumque, ubi altera dearum. Illae venit gerit ortum inpia ingens, femina forent tremulis; victor uror putares prolem . Fragmina si ubi dea genuit es viso quem, vetustas et quid, rex contulerat lentisciferumque insilit. Trepidus locutus corpora ignes! Nec enim fecere Nam tarda et alter non Ubi regni solebat Gentes viscera precor rogat In clausit patrem quamvis verbere Nunc ore mater armenta est lumine, quam ter Coniugis a cumque, amictu, atra nudos fugiunt Achivi geminos. Nec est Curibusque sperni saxoque et raptae, nulli! Tibi sub adest magnis agnovit quaecumque somnos flammas inpia concutiens, conscendit At cum prohibete. Tibi icta saevior simillimus loqui; sed herba ira erat viribus lumen pendeat modo senex porrigeret quaesita mea; arboreo.","title":"Orba formosa"},{"location":"about/#ululatibus-primo-renasci","text":"Pars edentem sulphura nati prodest, ego quo, atrae inpius saxum. Et e ramos volucrem volenti ritu, inserere domos fronti famam. Sidere esse lumina Orphne Somnia donisque liquerat dixit et agrestis ergo, mente tamen. Est est Domoque vocat Tibi tecum sistetur Cecrope natos Phegeius Dolos fulmina Vita sub violave quae","title":"Ululatibus primo renasci"},{"location":"about/#pavidus-o-in-alis","text":"Simul si nec fallebat fraterno parenti in culpa matertera inde planxere enim Ciconum ne tempora meorum praecipitem mersit. Obscuraque daedalus Iunone; dulce in quantas erat sumit animae cui undis ad latronis, inpune et. Sagitta virgo turpius: ursos quis spiritus pisa vatis fatale, mortis vidisti, in, sed Hactenus pendet. In nubimus qui, Somnia cecidisse alantqueimperat quaeritur contende alimentaque et in resolvit habentem tantos putes: femina levis suus? Labori et meorum fuissem membris. Omnes Proserpina vocassent apro excubias ulvaeque Pleuronius et mugiat modo placat quod. Ter thalamos relinque, putes tegis partem ferat mihi , nullamque saepe, virosque caelesti sub et. Latratibus notissima violenta ad veri , sub de est, de formosi adspicere plus orbem conviciaque videtur. Frater dederat nunc, meritoque urebat et sed; genialis draconi certamine flagrantem tabo; Mermeros. Lumen tu sed rapinae lotos hominis ter, erat dempto in aevum nimiumque ex neci simul manusque Tarpeias callem insistere. Virgo non quibus tamen rursus, Patareaque, nec reppulit putes et septem.","title":"Pavidus o in alis"},{"location":"code-api/","text":"DeepClaw Functions /utils read_yaml Classes ./driver ArmController Base class for multiple arm controller extensions. UR10eController Extends the ArmCotroller class with additional separate functions for Universal Robot Arm 10e. URConnector A wrapper of socket connection for UR arm. Frame Realsense /utils JsonEncoder","title":"deepclaw"},{"location":"code-api/#deepclaw","text":"","title":"DeepClaw"},{"location":"code-api/#functions","text":"/utils read_yaml","title":"Functions"},{"location":"code-api/#classes","text":"./driver ArmController Base class for multiple arm controller extensions. UR10eController Extends the ArmCotroller class with additional separate functions for Universal Robot Arm 10e. URConnector A wrapper of socket connection for UR arm. Frame Realsense /utils JsonEncoder","title":"Classes"},{"location":"data/","text":"","title":"Common Dataset"},{"location":"devnote/","text":"Developer Note This documentation is edited using MkDocs . Please install it before editing.","title":"For Developers"},{"location":"devnote/#developer-note","text":"This documentation is edited using MkDocs . Please install it before editing.","title":"Developer Note"},{"location":"install/","text":"The DeepClaw Benchmark The DeepClaw is a benchmarking model zoo that functions as a Reconfigurable Robotic Manipulation System for Robot Learning. The main homepage for Julia can be found at deepclaw.ancorasir.com . This is the GitHub repository of DeepClaw source code, including instructions for installing and using DeepClaw, below. Quick Start As of now, DeepClaw framework has been tested with Python 3.7 and Ubuntu 18.04 LTS. Pre-requirements In the current release, support is provided for a baseline setup with UR10e, HandE, and RealSense D435. The dependencies of DeepClaw are showed below: RealSense SDK Before using DeepClaw, please ensure you have installed all above packages. Install using pip Install DeepClaw using pip in virtual environment. $ python3 -m pip install DeepClaw Install using Docker Update later. Install from source Clone repository from Github. $ git clone https://github.com/bionicdl-sustech/DeepClawBenchmark.git $ cd DeepClawBenchmark Install DeepClaw. $ python3 setup.py install","title":"Installation"},{"location":"install/#the-deepclaw-benchmark","text":"The DeepClaw is a benchmarking model zoo that functions as a Reconfigurable Robotic Manipulation System for Robot Learning. The main homepage for Julia can be found at deepclaw.ancorasir.com . This is the GitHub repository of DeepClaw source code, including instructions for installing and using DeepClaw, below.","title":"The DeepClaw Benchmark"},{"location":"install/#quick-start","text":"As of now, DeepClaw framework has been tested with Python 3.7 and Ubuntu 18.04 LTS.","title":"Quick Start"},{"location":"install/#pre-requirements","text":"In the current release, support is provided for a baseline setup with UR10e, HandE, and RealSense D435. The dependencies of DeepClaw are showed below: RealSense SDK Before using DeepClaw, please ensure you have installed all above packages.","title":"Pre-requirements"},{"location":"install/#install-using-pip","text":"Install DeepClaw using pip in virtual environment. $ python3 -m pip install DeepClaw","title":"Install using pip"},{"location":"install/#install-using-docker","text":"Update later.","title":"Install using Docker"},{"location":"install/#install-from-source","text":"Clone repository from Github. $ git clone https://github.com/bionicdl-sustech/DeepClawBenchmark.git $ cd DeepClawBenchmark Install DeepClaw. $ python3 setup.py install","title":"Install from source"},{"location":"model/","text":"Model Zoo XXXX","title":"Model Zoo"},{"location":"model/#model-zoo","text":"XXXX","title":"Model Zoo"},{"location":"overview/","text":"DeepClaw Documentation The DeepClaw is a benchmarking model zoo that functions as a Reconfigurable Robotic Manipulation System for Robot Learning. Modern systems for robotic manipulation has gradually evolved to a common configuration of a cell station where multiple hardware and software are usually integrated to perform programmable tasks, such as object manipulation, human-robot interaction, and industrial collaboration. The challenge behind most manipulation-based robotic research and application is usually hidden by the growing complexity when integrating related hardware, such as manipulators, grippers, sensors, and environment with objects for manipulation, as well as software, such as algorithms, APIs, drivers, etc., towards a functional robot system. It is widely recognized among industrial robotics that the total cost of integration is usually at the same level as the price tage of the hardware itself, and may become even higher at many scenarios, where the market of robotic integration is actually slightly higher than the selling of robotic hardware itself. For academic research, there are several alternative solutions before DeepClaw. Commercial service providers for robotic integration : This is a common solution that may require the least of effort at the beginning. But in the end, may not be suitable for research purpose at some point where further developement may not be possible or just too expensive. Robot Operating System (ROS) : The development of ROS contributed significantly to the growing adoption of robotic technologies in research and applications, which provides a standardized interface for communication among various robotic devices and software. However, the scope of the ROS is so broad with a relatively steep learning curve for most users, learners, and practioners to get started. Do-it-yourself (DIY) : This is also a common solution which usually starts with the dedicated software provided by the hardware supplier, or if the robotic device is developed by the research team from the ground up. With the growing adoption of collaborative robotic devices, the barrier of usage becomes much lower than before. With a growing interest in robot learning, where learning-based methods are applied to robotic system to solve advanced manipulation tasks towards human-robot interaction. Several questions becomes imminent for researchers and practitioners: Is there a \"common\" setup of robotic manipulation station? By what \"standard\" should I pick the specific hardware to build my robot learning station? Is there a \"pipeline\" of intergration so that I can get the system up-and-running and perform real-world experiment? While there is no \"correct\" answers to questions such as above, these challenges are commonly faced by many researchers already in this field, or have the interest to experiement with robot learning, which give birth to the development of DeepClaw. Based on the existing collection of robotic hardware at the Bionic Design and Learning Lab at the Southern University of Science and Technology, we aim to develop a sharable and reproducible framework of Robot Manipulation System based on reviews of common setups of robot learning stations in recent research, aiming at building up a pool of hardware benchmarking system that facilitates a comparable metric for advanced robot learning algorithms, which is not yet explored in the current research (and also challenging). DeepClaw is hardware-centric model zoo towards a common design to benchmark robot stations through a Robotic Manipulation System, which involves a mechanical design of the DeepClaw Station, a growing collection of drivers that supports various robotic hardware, a cloud architecture that supports training and inference of robot learning experiments, and a collection of game objects for task representation. Version History v2.0: updated with server-client support. v1.5: early design used in XXX. v1.0: early design used in XXX. Mechanical Design of the DeepClaw Station After reviewing the robot station designs presented in several recent publications on robot manipulation learning, we proposed the following design using a standardized 9090 extrusion from global supplier such as Misumi and a few parts that can be easily machined by local machine shops to build up the system. The design can be accessed interactively through here. Feel free to contact us if you experience trouble sourcing the design, or have any suggestions to improve. AluExtru : 9090 series aluminium extrusion profiles from global supperlier such as Misumi. Misumi Part#: HFS8-9090-630-TPW, HFS8-9090-540-TPW, HFS8-9090-450-TPW. AluPlate : simple design peg-hole style aluminium plate for reliable connection, can be easily machined by you local shop. S2x4, S2x5. FlangeX : simple flange designs that supports heavy load connection, can be easily machined by your local shop. FlangeRobot: Supports Franka, UR, and AUBO in one design, as of now. FlangeFoot: Support the adjustable wheel. FlangeTube: Support RealSense D400 series, as of now. Others : minimum accessories towards flexibility, robustness, and safety. Tube: where cameras are mounted, can be easily modified based on your camera needs. Handle (Part#: GHHD28-A): for ease of handling and safe usage. Adjustable Wheel (GD-80F): for sturdiness and mobility. TableTop (630x540): aluminium with CNC machined, threaded peg-holes, modify as you need. Supported Robotic Hardware Here lists the robotic hardware that are currently supported by the DeepClaw station, all tested with a reasonable coverage of common adoption with a reasonable price range and functional support in most open-source repositories. Pipeline of Integration For further details, please refer to our recent publication at AIM 2020. Architecture of Integration Game-based Objects for Manipulation Citation arXiv For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Overview"},{"location":"overview/#deepclaw-documentation","text":"The DeepClaw is a benchmarking model zoo that functions as a Reconfigurable Robotic Manipulation System for Robot Learning. Modern systems for robotic manipulation has gradually evolved to a common configuration of a cell station where multiple hardware and software are usually integrated to perform programmable tasks, such as object manipulation, human-robot interaction, and industrial collaboration. The challenge behind most manipulation-based robotic research and application is usually hidden by the growing complexity when integrating related hardware, such as manipulators, grippers, sensors, and environment with objects for manipulation, as well as software, such as algorithms, APIs, drivers, etc., towards a functional robot system. It is widely recognized among industrial robotics that the total cost of integration is usually at the same level as the price tage of the hardware itself, and may become even higher at many scenarios, where the market of robotic integration is actually slightly higher than the selling of robotic hardware itself. For academic research, there are several alternative solutions before DeepClaw. Commercial service providers for robotic integration : This is a common solution that may require the least of effort at the beginning. But in the end, may not be suitable for research purpose at some point where further developement may not be possible or just too expensive. Robot Operating System (ROS) : The development of ROS contributed significantly to the growing adoption of robotic technologies in research and applications, which provides a standardized interface for communication among various robotic devices and software. However, the scope of the ROS is so broad with a relatively steep learning curve for most users, learners, and practioners to get started. Do-it-yourself (DIY) : This is also a common solution which usually starts with the dedicated software provided by the hardware supplier, or if the robotic device is developed by the research team from the ground up. With the growing adoption of collaborative robotic devices, the barrier of usage becomes much lower than before. With a growing interest in robot learning, where learning-based methods are applied to robotic system to solve advanced manipulation tasks towards human-robot interaction. Several questions becomes imminent for researchers and practitioners: Is there a \"common\" setup of robotic manipulation station? By what \"standard\" should I pick the specific hardware to build my robot learning station? Is there a \"pipeline\" of intergration so that I can get the system up-and-running and perform real-world experiment? While there is no \"correct\" answers to questions such as above, these challenges are commonly faced by many researchers already in this field, or have the interest to experiement with robot learning, which give birth to the development of DeepClaw. Based on the existing collection of robotic hardware at the Bionic Design and Learning Lab at the Southern University of Science and Technology, we aim to develop a sharable and reproducible framework of Robot Manipulation System based on reviews of common setups of robot learning stations in recent research, aiming at building up a pool of hardware benchmarking system that facilitates a comparable metric for advanced robot learning algorithms, which is not yet explored in the current research (and also challenging). DeepClaw is hardware-centric model zoo towards a common design to benchmark robot stations through a Robotic Manipulation System, which involves a mechanical design of the DeepClaw Station, a growing collection of drivers that supports various robotic hardware, a cloud architecture that supports training and inference of robot learning experiments, and a collection of game objects for task representation.","title":"DeepClaw Documentation"},{"location":"overview/#version-history","text":"v2.0: updated with server-client support. v1.5: early design used in XXX. v1.0: early design used in XXX.","title":"Version History"},{"location":"overview/#mechanical-design-of-the-deepclaw-station","text":"After reviewing the robot station designs presented in several recent publications on robot manipulation learning, we proposed the following design using a standardized 9090 extrusion from global supplier such as Misumi and a few parts that can be easily machined by local machine shops to build up the system. The design can be accessed interactively through here. Feel free to contact us if you experience trouble sourcing the design, or have any suggestions to improve. AluExtru : 9090 series aluminium extrusion profiles from global supperlier such as Misumi. Misumi Part#: HFS8-9090-630-TPW, HFS8-9090-540-TPW, HFS8-9090-450-TPW. AluPlate : simple design peg-hole style aluminium plate for reliable connection, can be easily machined by you local shop. S2x4, S2x5. FlangeX : simple flange designs that supports heavy load connection, can be easily machined by your local shop. FlangeRobot: Supports Franka, UR, and AUBO in one design, as of now. FlangeFoot: Support the adjustable wheel. FlangeTube: Support RealSense D400 series, as of now. Others : minimum accessories towards flexibility, robustness, and safety. Tube: where cameras are mounted, can be easily modified based on your camera needs. Handle (Part#: GHHD28-A): for ease of handling and safe usage. Adjustable Wheel (GD-80F): for sturdiness and mobility. TableTop (630x540): aluminium with CNC machined, threaded peg-holes, modify as you need.","title":"Mechanical Design of the DeepClaw Station"},{"location":"overview/#supported-robotic-hardware","text":"Here lists the robotic hardware that are currently supported by the DeepClaw station, all tested with a reasonable coverage of common adoption with a reasonable price range and functional support in most open-source repositories.","title":"Supported Robotic Hardware"},{"location":"overview/#pipeline-of-integration","text":"For further details, please refer to our recent publication at AIM 2020.","title":"Pipeline of Integration"},{"location":"overview/#architecture-of-integration","text":"","title":"Architecture of Integration"},{"location":"overview/#game-based-objects-for-manipulation","text":"","title":"Game-based Objects for Manipulation"},{"location":"overview/#citation","text":"arXiv For full documentation visit mkdocs.org .","title":"Citation"},{"location":"overview/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"overview/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"pipeline/","text":"Standardized Process in DeepClaw In DeepClaw, a sub-task is defined by a pipeline of modules, including segmentation, recognition, grasp planning, and motion planning, as shown in above figure. The pipeline takes color/depth images, force feedback, hardware limitation, and environment information as input and gives actions to the manipulation system and pushes data and results to data monitor. Segmentation Segmentation and recognition involve analyzing information gained from the perception system. Segmentation is the process that robot cell collecting environment information and representing spatial information of the target objects by using perception algorithms. The output of the segmentation module can be pixel-wise masks or bounding boxes. DeepClaw includes basic segmentation based on contour and edge detection in OpenCV [28]. Recognition Recognition is the process of extracting features of the target object beyond location information. In this step, the robot cell infers the category of the target object by applying specific methods, such as support vector machine(SVM) and convolutional neural network [29]. Some of the end-to-end neural networks infer the location and category of the target object at the same time. Grasping Planning Grasping planning aims to find the optimal pose for the robot arm and end-effect to approach the target objects, which is highly dependent on both the end-effector and the objects. Recent years, research interests have shifted from analytic method [32, [33] to data-driven method [2], [12], [34]. DeepClaw has implemented an end-to-end grasp planning model based on fully convolutional AlexNet, which was trained on 5000 random grasps with labels. Motion Planning Motion planning utilizes information above, such as grasping pose, force sensor data, constrain of the robot system, and limitation of working space, to obtain collision-free trajectories. Currently, waypoint-based motion planning is used through our tasks. For UR5 and UR10e, we utilize the movej command implemented in UR\u2019s controller to plan and execute a path between waypoints. For Franka, we utilize a fourthorder motion generator in the joint space provided by the libfranka software.","title":"DeepClaw Pipeline"},{"location":"pipeline/#standardized-process-in-deepclaw","text":"In DeepClaw, a sub-task is defined by a pipeline of modules, including segmentation, recognition, grasp planning, and motion planning, as shown in above figure. The pipeline takes color/depth images, force feedback, hardware limitation, and environment information as input and gives actions to the manipulation system and pushes data and results to data monitor.","title":"Standardized Process in DeepClaw"},{"location":"pipeline/#segmentation","text":"Segmentation and recognition involve analyzing information gained from the perception system. Segmentation is the process that robot cell collecting environment information and representing spatial information of the target objects by using perception algorithms. The output of the segmentation module can be pixel-wise masks or bounding boxes. DeepClaw includes basic segmentation based on contour and edge detection in OpenCV [28].","title":"Segmentation"},{"location":"pipeline/#recognition","text":"Recognition is the process of extracting features of the target object beyond location information. In this step, the robot cell infers the category of the target object by applying specific methods, such as support vector machine(SVM) and convolutional neural network [29]. Some of the end-to-end neural networks infer the location and category of the target object at the same time.","title":"Recognition"},{"location":"pipeline/#grasping-planning","text":"Grasping planning aims to find the optimal pose for the robot arm and end-effect to approach the target objects, which is highly dependent on both the end-effector and the objects. Recent years, research interests have shifted from analytic method [32, [33] to data-driven method [2], [12], [34]. DeepClaw has implemented an end-to-end grasp planning model based on fully convolutional AlexNet, which was trained on 5000 random grasps with labels.","title":"Grasping Planning"},{"location":"pipeline/#motion-planning","text":"Motion planning utilizes information above, such as grasping pose, force sensor data, constrain of the robot system, and limitation of working space, to obtain collision-free trajectories. Currently, waypoint-based motion planning is used through our tasks. For UR5 and UR10e, we utilize the movej command implemented in UR\u2019s controller to plan and execute a path between waypoints. For Franka, we utilize a fourthorder motion generator in the joint space provided by the libfranka software.","title":"Motion Planning"},{"location":"robot/","text":"Robot Library The DeepClaw cell, shown on the bottom of the following figure, is a self-contained robot cell for manipulation tasks, including an assembled station, a robot arm, a robot end-effector, a visual sensor. Thanks to the standardized design, we are able to buy the components and assemble the robot station quickly. The hardware setup is defined in the software part of DeepClaw through configuration files. The same task should be easily reproduced on different hardware setups if the configuration file is adequately defined. FRANKA+Two-finger Gripper+Realsense D435 Update later. UR5+RG6+Realsense D435 Update later. UR10e-HandE-Kinect Azure Update later.","title":"Robot Library"},{"location":"robot/#robot-library","text":"The DeepClaw cell, shown on the bottom of the following figure, is a self-contained robot cell for manipulation tasks, including an assembled station, a robot arm, a robot end-effector, a visual sensor. Thanks to the standardized design, we are able to buy the components and assemble the robot station quickly. The hardware setup is defined in the software part of DeepClaw through configuration files. The same task should be easily reproduced on different hardware setups if the configuration file is adequately defined.","title":"Robot Library"},{"location":"robot/#frankatwo-finger-gripperrealsense-d435","text":"Update later.","title":"FRANKA+Two-finger Gripper+Realsense D435"},{"location":"robot/#ur5rg6realsense-d435","text":"Update later.","title":"UR5+RG6+Realsense D435"},{"location":"robot/#ur10e-hande-kinect-azure","text":"Update later.","title":"UR10e-HandE-Kinect Azure"},{"location":"sim2real/","text":"Sim-2-Real XXXX","title":"Sim-2-Real"},{"location":"sim2real/#sim-2-real","text":"XXXX","title":"Sim-2-Real"},{"location":"task/","text":"Task Family In DeepClaw, a manipulation task involves three hierarchical concepts: task, sub-task, functionality module. A task protocol should clearly define the main purpose of the task, the target objects, the robot, and hardware setup, procedures to fulfill the task and execution constraints [19]. Each task may consist of several repetitive sub-tasks. A pipeline of functional modules can accomplish each sub-task. The most similarity between game and dexterous manipulations enable reproducible experiments in various environment. All of the game manipulation tasks can be classified from two different perspectives: spatial reasoning and temporal reasoning. Compared with human daily dexterous manipulations, game manipulations have a noticeable distinction in spatial and temporal dimensions. \"Jigsaw Puzzle,\" for example, requires a meaningful pattern at finally by placing certain pieces in settled spatial position and orientation using robot cell. We can summarize that \"Jigsaw Puzzle\" focuses on spatial reasoning rather than temporal reasoning sine chronological operations to finish the puzzle are needless during the whole placing process. \"Tic-tac-toe Game\" is the contrary that emphasizes moving chess chronologically rather than its spatial position and orientation (distinguish the type of pieces rather than each piece individual). Claw machine is another popular game that involves picking and placing to clear the toy tray. We hypothesize that both robot cells and intelligent algorithms lead to performance differences when executing game manipulation tasks. Tic-Tac-Toe: Board Games as Adversarial Interaction Tic-Tac-Toe game is a temporal reasoning related task, which required two players moving pieces alternately. To simplify this game as a baseline, the two players use the same placing strategy, namely the Minimax algorithm with depth 3, and are both executed by the robot arm. We use green and blue cubes from Yale-CMU-Berkeley objects set [19] representing two types of pieces. At the start of the game, 3\u00d73 checkerboards printed on an A4 paper is placed in front of the robot base, and the two types of pieces are lined on the left and right side of the chessboard as shown in Fig 4(a). The task is to pick a type of piece and place it on one of nine boxes on the checkerboard in turns until one player wins or ends with a tie. Claw Machine: End-to-End Manipulation Benchmarking This benchmark measures the performance of a learned policy for predicting robust grasps over different robot cells. At the start of the task, a 60cm\u00d770cm white bin stuffed by eight soft toys and an empty 30cm\u00d740cm blue bin are placed side by side on the table top as shown in the following figure. The task is to transport the toys to the blue bin one by one until clearing the white bin. We restrict the gripper to grasp vertically, allowing only rotations along the z-axis of the robot base. Jigsaw Puzzle: Tiling Game for Modular Benchmarking A jigsaw puzzle is a tiling game that requires the assembly of often oddly shaped interlocking and tessellating pieces. The jigsaw set used in this paper contains four thin wooden pieces with an image printed on one side and can form a 10.2cm\u00d710.2cm picture when they are correctly assembled. We use a suction cup to complete the task on all three robot cells as the jigsaw piece is only 5 mm thick and is too challenging for grippers. At the start of the task, the four pieces are randomly placed on the table top, as shown in Fig. 6(a). The task is to detect and pick one jigsaw piece at a time and place it at the required location according to its shape and texture information, and finally assemble all the four pieces into one whole piece. We restrict the gripper to pick vertically, allowing only rotations along the z-axis of the robot base.","title":"Task Family"},{"location":"task/#task-family","text":"In DeepClaw, a manipulation task involves three hierarchical concepts: task, sub-task, functionality module. A task protocol should clearly define the main purpose of the task, the target objects, the robot, and hardware setup, procedures to fulfill the task and execution constraints [19]. Each task may consist of several repetitive sub-tasks. A pipeline of functional modules can accomplish each sub-task. The most similarity between game and dexterous manipulations enable reproducible experiments in various environment. All of the game manipulation tasks can be classified from two different perspectives: spatial reasoning and temporal reasoning. Compared with human daily dexterous manipulations, game manipulations have a noticeable distinction in spatial and temporal dimensions. \"Jigsaw Puzzle,\" for example, requires a meaningful pattern at finally by placing certain pieces in settled spatial position and orientation using robot cell. We can summarize that \"Jigsaw Puzzle\" focuses on spatial reasoning rather than temporal reasoning sine chronological operations to finish the puzzle are needless during the whole placing process. \"Tic-tac-toe Game\" is the contrary that emphasizes moving chess chronologically rather than its spatial position and orientation (distinguish the type of pieces rather than each piece individual). Claw machine is another popular game that involves picking and placing to clear the toy tray. We hypothesize that both robot cells and intelligent algorithms lead to performance differences when executing game manipulation tasks.","title":"Task Family"},{"location":"task/#tic-tac-toe-board-games-as-adversarial-interaction","text":"Tic-Tac-Toe game is a temporal reasoning related task, which required two players moving pieces alternately. To simplify this game as a baseline, the two players use the same placing strategy, namely the Minimax algorithm with depth 3, and are both executed by the robot arm. We use green and blue cubes from Yale-CMU-Berkeley objects set [19] representing two types of pieces. At the start of the game, 3\u00d73 checkerboards printed on an A4 paper is placed in front of the robot base, and the two types of pieces are lined on the left and right side of the chessboard as shown in Fig 4(a). The task is to pick a type of piece and place it on one of nine boxes on the checkerboard in turns until one player wins or ends with a tie.","title":"Tic-Tac-Toe: Board Games as Adversarial Interaction"},{"location":"task/#claw-machine-end-to-end-manipulation-benchmarking","text":"This benchmark measures the performance of a learned policy for predicting robust grasps over different robot cells. At the start of the task, a 60cm\u00d770cm white bin stuffed by eight soft toys and an empty 30cm\u00d740cm blue bin are placed side by side on the table top as shown in the following figure. The task is to transport the toys to the blue bin one by one until clearing the white bin. We restrict the gripper to grasp vertically, allowing only rotations along the z-axis of the robot base.","title":"Claw Machine: End-to-End Manipulation Benchmarking"},{"location":"task/#jigsaw-puzzle-tiling-game-for-modular-benchmarking","text":"A jigsaw puzzle is a tiling game that requires the assembly of often oddly shaped interlocking and tessellating pieces. The jigsaw set used in this paper contains four thin wooden pieces with an image printed on one side and can form a 10.2cm\u00d710.2cm picture when they are correctly assembled. We use a suction cup to complete the task on all three robot cells as the jigsaw piece is only 5 mm thick and is too challenging for grippers. At the start of the task, the four pieces are randomly placed on the table top, as shown in Fig. 6(a). The task is to detect and pick one jigsaw piece at a time and place it at the required location according to its shape and texture information, and finally assemble all the four pieces into one whole piece. We restrict the gripper to pick vertically, allowing only rotations along the z-axis of the robot base.","title":"Jigsaw Puzzle: Tiling Game for Modular Benchmarking"}]}